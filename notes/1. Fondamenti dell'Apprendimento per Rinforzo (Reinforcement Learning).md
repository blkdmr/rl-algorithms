
L'Apprendimento per Rinforzo, comunemente noto come **Reinforcement Learning (RL)**, rappresenta un terzo paradigma nel campo del Machine Learning, collocandosi in una posizione intermedia tra l'apprendimento supervisionato e quello non supervisionato. Sebbene utilizzi tecniche consolidate dell'apprendimento supervisionato, come le reti neurali profonde per l'approssimazione delle funzioni, la discesa stocastica del gradiente e la backpropagation, il modo in cui i dati vengono elaborati e l'obiettivo finale sono profondamente diversi.

A differenza dell'apprendimento supervisionato, in cui il modello viene addestrato su coppie input-output predefinite (etichette), nel RL non esiste un "insegnante" che indichi esplicitamente l'azione corretta da compiere in ogni situazione. L'obiettivo è invece dotare un agente della capacità di apprendere autonomamente come interagire con un ambiente dinamico per massimizzare una ricompensa cumulativa nel tempo.

## L'Analogia del Topo Robot e l'Ambiente del Labirinto

Per comprendere meglio il funzionamento del RL, si può immaginare un topo robot all'interno di un labirinto. L'ambiente contiene elementi positivi, come il cibo, ed elementi negativi, come scosse elettriche. In questo scenario, il topo non riceve istruzioni passo-passo su quali svolte effettuare; deve invece esplorare l'ambiente e ricevere segnali di feedback sotto forma di **ricompense** ($r$).

Il sistema di ricompensa è fondamentale:
* **Positivo:** Quando l'agente raccoglie cibo.
* **Negativo:** Quando l'agente riceve una scossa elettrica.
* **Neutro:** Quando non accade nulla di rilevante.

Attraverso l'osservazione dei risultati delle proprie azioni, l'agente impara a correlare determinati comportamenti a esiti favorevoli o sfavorevoli. Ad esempio, il topo potrebbe accettare una piccola scossa elettrica nel breve termine se questa è necessaria per raggiungere una grande quantità di cibo nel lungo termine. Il fine ultimo dell'agente è massimizzare il valore atteso della somma delle ricompense future:

$$G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}$$

Dove $\gamma \in [0, 1]$ è il fattore di sconto che determina l'importanza delle ricompense future rispetto a quelle immediate.

---

## Complicazioni e Sfide nel Reinforcement Learning

Il RL è considerato significativamente più complesso rispetto ad altri approcci di apprendimento automatico a causa di diverse sfide intrinseche legate alla natura stessa dell'interazione agente-ambiente.

### Dati Non-IID e Dipendenza dal Comportamento
Nell'apprendimento supervisionato, i dati sono solitamente assunti come **IID** (*independent and identically distributed*). Nel RL, questa condizione viene meno perché le osservazioni dipendono direttamente dal comportamento dell'agente. Se l'agente adotta una strategia inefficiente, riceverà solo feedback negativi e le sue osservazioni saranno limitate a una porzione ristretta e "fallimentare" dello spazio degli stati. Questo può portare a una percezione errata dell'ambiente, dove l'agente potrebbe concludere erroneamente che non esista alcuna via per ottenere una ricompensa maggiore.

### Il Dilemma tra Esplorazione ed Sfruttamento (Exploration vs. Exploitation)
Una delle sfide fondamentali è trovare l'equilibrio tra **exploitation** (sfruttare le conoscenze già acquisite per ottenere ricompense sicure) e **exploration** (esplorare nuove azioni per scoprire strategie potenzialmente migliori). Un eccesso di esplorazione può portare a una perdita immediata di ricompense e al rischio di dimenticare le strategie apprese, mentre un eccesso di sfruttamento impedisce all'agente di scoprire percorsi ottimali che non ha ancora provato.

### Ricompensa Ritardata
A differenza di altri modelli in cui il feedback è immediato, nel RL la ricompensa può essere fortemente ritardata rispetto all'azione che l'ha causata. In un gioco come gli scacchi, una singola mossa forte fatta a metà partita può determinare la vittoria finale molti turni dopo. Identificare quali azioni passate siano responsabili del successo o del fallimento attuale è noto come il **problema dell'assegnazione del credito** (*credit assignment problem*).

---

## Entità Fondamentali e Formalismi

L'architettura del RL si basa sull'interazione continua tra due entità principali:

1.  **Agente (Agent):** L'entità che prende decisioni e compie azioni (es. il topo robot).
2.  **Ambiente (Environment):** Tutto ciò con cui l'agente interagisce (es. il labirinto).

L'interazione avviene attraverso tre canali di comunicazione principali:
* **Azioni ($a$):** Le scelte effettuate dall'agente (es. girare a destra o sinistra).
* **Osservazioni o Stati ($s$):** Ciò che l'agente percepisce dell'ambiente in un dato momento.
* **Ricompensa ($r$):** Il feedback numerico inviato dall'ambiente in risposta a un'azione.

Nonostante la complessità e la natura dinamica di questi elementi, il RL fornisce un quadro matematico rigoroso per risolvere problemi decisionali complessi che non potrebbero essere affrontati tramite la semplice programmazione manuale o la supervisione diretta.

# Componenti Fondamentali del Reinforcement Learning

Il paradigma del Reinforcement Learning si basa sull'interazione dinamica tra un agente e il suo ambiente attraverso canali di comunicazione specifici. Di seguito vengono analizzati i pilastri teorici che definiscono questo sistema.

---

## La Ricompensa (Reward)

La ricompensa, indicata generalmente con $r$, è un valore scalare che l'agente riceve periodicamente dall'ambiente. Rappresenta il feedback principale e ha lo scopo di comunicare all'agente l'efficacia del suo comportamento. 

Il termine **rinforzo** deriva proprio dal fatto che questo segnale deve rafforzare, in senso positivo o negativo, le azioni compiute. La ricompensa è un concetto locale e immediato: ottenere un valore elevato in un istante $t$ non garantisce che le conseguenze a lungo termine siano favorevoli. L'obiettivo dell'agente non è massimizzare la ricompensa istantanea, ma la **ricompensa accumulata** (o *ritorno*) nel tempo.


### Esempi di Sistemi di Ricompensa
* **Trading finanziario:** Il profitto o la perdita realizzata in seguito a operazioni di compravendita.
* **Giochi (Scacchi/Videogiochi):** Il risultato finale (vittoria, pareggio, sconfitta) o l'incremento del punteggio (derivata del punteggio totale).
* **Sistemi biologici:** Il rilascio di dopamina nel cervello in risposta a stimoli positivi (cibo, riproduzione, sicurezza).
* **Addestramento animale:** Un premio commestibile dato al cane quando esegue correttamente un comando.

---

## L'Agente e l'Ambiente

L'**Agente** è l'entità decisionale che interagisce con il mondo esterno. Nella maggior parte delle applicazioni pratiche, l'agente è un software progettato per risolvere un problema specifico (es. un algoritmo di trading, un bot per scacchi o un sistema di navigazione web).

L'**Ambiente** rappresenta tutto ciò che è esterno all'agente. In senso teorico, l'ambiente comprende l'intero universo, ma nelle applicazioni pratiche viene limitato ai fattori che influenzano direttamente il compito dell'agente. La comunicazione tra agente e ambiente è strettamente limitata a tre elementi: **azioni**, **osservazioni** e **ricompense**.

---

## Azioni (Actions)

Le azioni sono le manovre che l'agente può compiere per influenzare lo stato dell'ambiente. In RL, le azioni si dividono in due categorie principali:

1.  **Azioni Discrete:** Un insieme finito e mutuamente esclusivo di opzioni (es. muovere a destra o a sinistra, comprare o vendere).
2.  **Azioni Continue:** Azioni che assumono valori numerici in un intervallo (es. l'angolo di sterzata di un'auto o la quantità di potenza applicata a un motore).

---

## Osservazioni vs Stato (Observations vs State)

È fondamentale distinguere tra ciò che l'agente percepisce e la realtà effettiva dell'ambiente:

* **Stato ($s$):** Rappresenta la configurazione completa e interna dell'ambiente. Spesso è inosservabile nella sua interezza (es. tutti i parametri che influenzano il mercato azionario globale).
* **Osservazione ($o$):** È il sottoinsieme di informazioni che l'ambiente fornisce all'agente. Le osservazioni possono essere rumorose, incomplete o contenere informazioni sulla ricompensa in forma offuscata (es. i pixel di un videogioco).

Il RL è progettato nativamente per gestire situazioni in cui l'agente non ha accesso allo stato perfetto dell'ambiente, operando sulla base di osservazioni parziali.

---

## Connessioni Interdisciplinari

Il Reinforcement Learning è un campo altamente flessibile che si colloca all'intersezione di diverse discipline scientifiche:

* **Machine Learning:** Fornisce gli strumenti (come le reti neurali) per apprendere dai dati.
* **Ingegneria (Controllo Ottimo):** Contribuisce con metodi per determinare sequenze di azioni che ottimizzano un risultato.
* **Neuroscienze e Psicologia:** Studiano come i sistemi biologici apprendono attraverso rinforzi, influenzando la struttura degli algoritmi.
* **Economia:** Analizza il processo decisionale in condizioni di incertezza e informazione imperfetta.
* **Matematica (Ricerca Operativa):** Si occupa della formalizzazione di sistemi idealizzati e della ricerca di condizioni di ottimalità.

# Fondamenti Teorici: Dai Processi di Markov al Reinforcement Learning

Per comprendere il Reinforcement Learning (RL) da un punto di vista scientifico, è necessario introdurre una gerarchia di formalismi matematici che descrivono come un sistema evolve nel tempo. Questi modelli, simili a una "matrioska" russa, partono dal caso più semplice per aggiungere progressivamente complessità:

1.  **Processo di Markov (MP)**: Solo stati e probabilità di transizione.
2.  **Processo di Ricompensa di Markov (MRP)**: Si aggiungono le ricompense agli stati.
3.  **Processo Decisionale di Markov (MDP)**: Si aggiungono le azioni che influenzano le transizioni.

---

## Il Processo di Markov (Markov Process)

Il **Processo di Markov** (o Catena di Markov) è un sistema che possiamo solo osservare, senza possibilità di influenzarlo. È definito da una sequenza di stati che cambiano secondo leggi dinamiche intrinseche.

### Lo Spazio degli Stati e la Storia
L'insieme di tutti i possibili stati in cui il sistema può trovarsi è chiamato **spazio degli stati** ($S$). Una sequenza di osservazioni effettuate nel tempo costituisce una **storia** o **catena**.
Esempio: In un modello meteorologico semplificato con $S = \{\text{Sole}, \text{Pioggia}\}$, una storia tipica potrebbe essere:
$$[s_0=\text{Sole}, s_1=\text{Sole}, s_2=\text{Pioggia}, s_3=\text{Sole}]$$

### La Proprietà di Markov
Un sistema è definito "Markoviano" se soddisfa la **proprietà di Markov**: la dinamica futura del sistema dipende esclusivamente dallo stato attuale e non dalla storia passata. In termini formali, la probabilità di passare allo stato successivo $s_{t+1}$ dipende solo da $s_t$:

$$P(s_{t+1} | s_t) = P(s_{t+1} | s_1, s_2, \dots, s_t)$$

Questa proprietà implica che ogni stato deve essere autosufficiente per descrivere il futuro del sistema. Se il futuro dipende da fattori passati non inclusi nello stato attuale, il modello deve essere ampliato (es. includendo la stagione nello stato meteorologico) per tornare a essere Markoviano.

### Matrice di Transizione
Le probabilità di passare da uno stato $i$ a uno stato $j$ sono racchiuse in una **matrice di transizione** ($T$), una matrice quadrata di dimensione $N \times N$ (dove $N$ è il numero di stati). Ogni cella $(i, j)$ contiene la probabilità $P_{i,j}$:

$$T = \begin{pmatrix} 
P_{1,1} & P_{1,2} & \dots & P_{1,n} \\
P_{2,1} & P_{2,2} & \dots & P_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
P_{n,1} & P_{n,2} & \dots & P_{n,n} 
\end{pmatrix}$$

Ogni riga della matrice deve sommare a $1$, poiché rappresenta una distribuzione di probabilità completa per lo stato successivo.



---

## Esempio: Il Modello "Office Worker"

Immaginiamo un lavoratore i cui stati possibili sono: **Casa**, **Computer**, **Caffè** e **Chat**. La dinamica del suo comportamento può essere rappresentata graficamente come un diagramma a nodi e archi, dove ogni arco indica la probabilità di transizione.

| Stato Attuale | Casa | Caffè | Chat | Computer |
| :--- | :--- | :--- | :--- | :--- |
| **Casa** | 0.6 | 0.4 | 0.0 | 0.0 |
| **Caffè** | 0.0 | 0.1 | 0.7 | 0.2 |
| **Chat** | 0.0 | 0.2 | 0.5 | 0.3 |
| **Computer** | 0.2 | 0.2 | 0.1 | 0.5 |

In questo modello, notiamo che la giornata inizia sempre con un caffè (transizione Casa $\to$ Caffè) e finisce solo quando il lavoratore è al computer (transizione Computer $\to$ Casa).



---

## Episodi e Stazionarietà

Nella pratica, raramente conosciamo la matrice di transizione a priori. Spesso disponiamo solo di **episodi**, ovvero campionamenti reali della catena di Markov:
* *Episodio 1:* Casa $\to$ Caffè $\to$ Chat $\to$ Computer $\to$ Casa
* *Episodio 2:* Computer $\to$ Chat $\to$ Caffè $\to$ Computer

La proprietà di Markov implica la **stazionarietà**: la distribuzione di probabilità delle transizioni per ogni stato non cambia nel tempo. Se esistessero fattori nascosti che modificano queste probabilità (non inclusi nelle osservazioni), il sistema sarebbe non-stazionario e il formalismo delle catene di Markov non sarebbe applicabile senza modifiche.

Possiamo stimare la matrice di transizione reale contando la frequenza delle transizioni osservate negli episodi; maggiore è il numero di dati, più accurata sarà la nostra stima del modello sottostante.

# Processi di Ricompensa di Markov (MRP)

Per avvicinarci al paradigma del Reinforcement Learning, dobbiamo evolvere il Processo di Markov (MP) aggiungendo la nozione di **ricompensa**. Un **Processo di Ricompensa di Markov (MRP)** è un'estensione in cui ogni transizione tra stati non è definita solo da una probabilità, ma anche da un valore scalare che quantifica la bontà di tale passaggio.

---

## Struttura della Ricompensa

In un MRP, la ricompensa può essere rappresentata in vari modi:
* **Matrice di Ricompensa:** Una matrice quadrata dove ogni cella $(i, j)$ indica il valore ottenuto passando dallo stato $i$ allo stato $j$.
* **Coppie Stato-Ricompensa:** Se il premio dipende esclusivamente dallo stato di destinazione, la rappresentazione può essere semplificata associando un valore direttamente a ogni stato.

Oltre alla ricompensa, introduciamo un parametro fondamentale: il **fattore di sconto** $\gamma$ (gamma), un numero compreso nell'intervallo $[0, 1]$.

---

## Il Ritorno e il Fattore di Sconto

In un MRP, l'osservazione non è più solo una catena di stati, ma una sequenza di coppie (stato, ricompensa). Per ogni episodio, definiamo il **ritorno** (o *gain*) al tempo $t$ come la somma pesata delle ricompense future:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

Il fattore $\gamma$ determina la lungimiranza dell'agente:
* **$\gamma = 0$:** L'agente è "miope"; il ritorno coincide solo con la ricompensa immediata.
* **$\gamma = 1$:** L'agente ha una visibilità perfetta sul futuro; ogni ricompensa futura ha lo stesso peso di quella attuale. Questo valore è utilizzabile principalmente in episodi finiti.
* **$0 < \gamma < 1$:** Rappresenta la configurazione standard (es. $0.9$ o $0.99$), dove le ricompense distanti nel tempo valgono progressivamente meno.



---

## Il Valore dello Stato (State Value)

Poiché il ritorno $G_t$ può variare significativamente anche partendo dallo stesso stato (a causa della natura stocastica delle transizioni), definiamo una quantità più stabile chiamata **Valore dello Stato** $V(s)$. Il valore di uno stato $s$ è il valore atteso (la media) del ritorno partendo da quello stato:

$$V(s) = \mathbb{E}[G | S_t = s]$$

### Esempio: Dilbert Reward Process (DRP)
Applichiamo questo concetto all'esempio del lavoratore d'ufficio, assegnando ricompense alle transizioni (es. Casa $\to$ Casa $= 1$, Computer $\to$ Chat $= -3$, ecc.).

Se impostiamo **$\gamma = 0$**, il valore di uno stato dipende solo dalle transizioni immediate pesate per la loro probabilità. Ad esempio, per lo stato *Chat*:
$$V(\text{chat}) = (-1 \cdot 0.5) + (2 \cdot 0.3) + (1 \cdot 0.2) = 0.3$$

Al contrario, se impostiamo **$\gamma = 1$** in un sistema senza stati terminali (come questo), il valore di ogni stato tenderebbe all'infinito, poiché continueremmo a sommare ricompense positive per un tempo indefinito. Questo evidenzia l'importanza di $\gamma < 1$ per mantenere i valori matematicamente trattabili in orizzonti temporali infiniti.

---

## Verso il Modello Completo

Nonostante l'MRP includa le ricompense, l'osservatore rimane ancora passivo: non può influenzare le transizioni per migliorare il proprio punteggio. Il passo finale per completare il quadro teorico del Reinforcement Learning consiste nell'introdurre la possibilità di scelta attraverso le **azioni**.

# Il Processo Decisionale di Markov (MDP)

L'ultimo tassello per completare il formalismo teorico del Reinforcement Learning è l'introduzione delle azioni, che trasforma un Processo di Ricompensa di Markov in un **Processo Decisionale di Markov (MDP)**. In questo scenario, l'agente smette di essere un osservatore passivo e acquisisce la capacità di influenzare l'evoluzione del sistema attraverso le proprie scelte.

## L'Integrazione delle Azioni e lo Spazio d'Azione

Per definire un MDP, è necessario innanzitutto stabilire un insieme finito di azioni $A$, denominato **spazio delle azioni**. Mentre nei modelli precedenti la dinamica era determinata esclusivamente dalle probabilità di transizione intrinseche del sistema, in un MDP la probabilità di passare da uno stato all'altro è condizionata dall'azione scelta dall'agente.

Dal punto di vista matematico, la matrice di transizione quadrata $|S| \times |S|$ evolve in una struttura tridimensionale, un "cuboide" di dimensioni $|S| \times |S| \times |A|$. Ogni cella di questa struttura rappresenta la probabilità:

$$P(s' | s, a) = P(S_{t+1} = s' | S_t = s, A_t = a)$$

Questa formula esprime la probabilità di approdare nello stato $s'$ partendo dallo stato $s$ dopo aver eseguito l'azione $a$. In questo modo, scegliendo un'azione specifica, l'agente può attivamente modificare le probabilità di distribuzione dei futuri stati target, orientando il sistema verso configurazioni più favorevoli.

## Stochasticità e Imperfezione delle Azioni

Un aspetto fondamentale degli MDP è che le azioni non portano necessariamente a un risultato deterministico. Nel mondo reale, l'esecuzione di un'azione può essere soggetta a rumore o imprevisti ambientali. 

Consideriamo l'esempio di un robot posizionato in una griglia $3 \times 3$. Se il robot può trovarsi in ogni cella con quattro possibili orientamenti (su, giù, destra, sinistra), lo spazio degli stati $S$ comprende $3 \times 3 \times 4 = 36$ stati totali. Se il robot tenta l'azione "avanti", potremmo avere una probabilità del $90\%$ di successo (spostamento nella cella successiva) e una probabilità del $10\%$ di fallimento (ad esempio, le ruote slittano e il robot rimane nella posizione attuale). Questa incertezza viene catturata perfettamente dalla matrice di transizione tridimensionale, che mappa ogni possibile esito di ogni azione in ogni stato.

## Estensione della Funzione di Ricompensa

Parallelamente alla transizione di stato, anche la struttura della ricompensa deve essere aggiornata per includere la dimensione delle azioni. In un MDP, la ricompensa non dipende più esclusivamente dallo stato di arrivo o dalla transizione, ma è funzione anche dell'azione compiuta. La funzione di ricompensa può essere espressa come:

$$R(s, a) = \mathbb{E}[R_{t+1} | S_t = s, A_t = a]$$

In alcuni modelli più complessi, la ricompensa può dipendere dalla triade stato iniziale, azione e stato finale, ovvero $R(s, a, s')$. Questo significa che l'agente riceve un feedback che valuta non solo "dove è arrivato", ma anche "come ha deciso di arrivarci". Ad esempio, in un sistema di guida autonoma, raggiungere la destinazione (stato) è positivo, ma farlo attraverso una sterzata brusca (azione) potrebbe comportare una penalità.

L'integrazione di questi elementi — stati, azioni, probabilità di transizione condizionate e ricompense — fornisce la base formale necessaria per affrontare il concetto più critico del Reinforcement Learning: la definizione di una strategia ottimale, nota come **policy**.

# La Politica (Policy)

L'ultimo elemento fondamentale per definire il comportamento di un agente in un Processo Decisionale di Markov è la **policy** ($\pi$). In termini semplici, la policy rappresenta l'insieme di regole che determinano le azioni dell'agente in base allo stato in cui si trova. È, in sostanza, il "cervello" dell'agente o la sua strategia decisionale.

---

## Definizione e Varietà di Comportamento

Anche in contesti molto semplici, come quello del robot nella griglia, un agente può adottare diverse strategie che portano a esiti differenti:
* **Comportamento deterministico:** Muoversi sempre in avanti ignorando ogni ostacolo.
* **Comportamento adattivo:** Cercare di aggirare gli ostacoli se l'azione precedente è fallita.
* **Comportamento casuale:** Scegliere un'azione a caso in ogni istante (modello del "robot ubriaco").

Dato che l'obiettivo primario nel Reinforcement Learning è accumulare il maggior ritorno possibile, l'identificazione di una **policy ottimale** è il compito centrale di ogni algoritmo di apprendimento.

---

## Formalizzazione Matematica

Formalmente, una policy è definita come una distribuzione di probabilità sulle azioni per ogni possibile stato:

$$\pi(a|s) = P[A_t = a | S_t = s]$$

Questa formula indica la probabilità che l'agente scelga l'azione $a$ quando si trova nello stato $s$. 

### Perché usare la probabilità?
L'uso di una distribuzione di probabilità anziché di un'azione fissa introduce un elemento di **stocasticità** nel comportamento dell'agente. Questo è fondamentale per favorire l'esplorazione dell'ambiente e per gestire scenari in cui una scelta fissa potrebbe essere facilmente predetta o contrastata. Una **policy deterministica** è semplicemente un caso particolare in cui una specifica azione ha probabilità pari a $1$ e tutte le altre hanno probabilità $0$.



---

## Riduzione da MDP a MRP

Un concetto teorico molto utile è la relazione tra MDP e MRP. Se la policy dell'agente è **fissa** (ovvero non cambia durante l'addestramento e restituisce sempre le stesse probabilità per gli stessi stati), l'intero sistema può essere semplificato. 

In questo caso, le dimensioni relative alle azioni nella matrice di transizione e nella matrice di ricompensa possono essere eliminate attraverso una media pesata basata sulle probabilità della policy. Di conseguenza, l'MDP si "riduce" a un Processo di Ricompensa di Markov (MRP), poiché l'elemento di scelta attiva dell'agente è diventato una componente fissa e prevedibile della dinamica del sistema.

$$P_{s,s'}^{\pi} = \sum_{a \in A} \pi(a|s) P(s'|s,a)$$

$$R_{s}^{\pi} = \sum_{a \in A} \pi(a|s) R(s,a)$$

Comprendere queste fondamenta teoriche è il passo cruciale per poter affrontare, nei capitoli successivi, le tecniche pratiche di Deep Reinforcement Learning che permettono di insegnare agli agenti come risolvere compiti complessi in modo autonomo.