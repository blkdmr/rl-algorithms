
Nel paradigma del Reinforcement Learning (RL), l'interazione tra l'agente e l'ambiente costituisce il nucleo fondamentale del processo di apprendimento. Questa interazione può essere scomposta in componenti strutturali che definiscono come le decisioni vengono prese e come il sistema evolve nel tempo.

## I Componenti Fondamentali

L'architettura del sistema si basa su due entità principali che operano in un ciclo continuo:

* **L'Agente**: Rappresenta l'entità decisionale, solitamente implementata tramite codice, che esegue una **policy** (politica). La policy determina quale azione intraprendere in ogni istante temporale $t$, basandosi sulle osservazioni ricevute.
* **L'Ambiente**: Comprende tutto ciò che è esterno all'agente. Ha il compito di fornire osservazioni e ricompense, modificando il proprio stato interno in risposta alle azioni dell'agente.

## Implementazione dell'Ambiente

Per comprendere la struttura logica, consideriamo un ambiente semplificato in Python. La classe `Environment` gestisce lo stato interno e le regole del "gioco".

### Stato e Osservazione
L'ambiente mantiene un contatore interno per limitare la durata dell'interazione. Il metodo `get_observation()` restituisce la percezione attuale dell'agente. In contesti complessi, l'osservazione è una funzione dello stato interno $s \in S$, dove $S$ è lo spazio degli stati. In questo esempio, l'osservazione è un vettore statico:

```python
def get_observation(self) -> List[float]:
    return [0.0, 0.0, 0.0]
````

### Azioni e Terminazione

L'agente deve poter conoscere le opzioni a sua disposizione tramite `get_actions()`. Sebbene in molti casi il set di azioni sia fisso, in scenari come il tris (tic-tac-toe), le azioni disponibili dipendono dallo stato corrente. La conclusione di un ciclo di interazioni è definita **episodio**. Un episodio termina quando il metodo `is_done()` restituisce `True`, segnalando che non è più possibile alcuna comunicazione.

### La Funzione di Transizione e Ricompensa

Il metodo `action()` è il cuore dell'ambiente. Esso elabora l'azione scelta dall'agente, aggiorna lo stato interno e restituisce una **ricompensa** ($R$).


```Python
def action(self, action: int) -> float:
    if self.is_done():
        raise Exception("Game is over")
    self.steps_left -= 1
    return random.random()
```

## Implementazione dell'Agente

L'agente ha una struttura più snella, focalizzata sull'accumulo del valore e sull'esecuzione della logica decisionale.

### Il Ciclo di Step

Il metodo principale `step()` definisce la sequenza operativa dell'agente durante un singolo istante temporale:

1. **Osservazione**: Ottenimento dello stato attuale dall'ambiente.
    
2. **Decisione**: Selezione di un'azione (in questo caso, casuale).
    
3. **Azione**: Invio dell'azione all'ambiente.
    
4. **Apprendimento/Accumulo**: Ricezione della ricompensa e aggiornamento del guadagno totale.
    

L'accumulo della ricompensa totale $G$ in un episodio può essere espresso matematicamente come la somma delle ricompense immediate:

$$G = \sum_{t=0}^{T} R_t$$

Dove $T$ è l'istante finale dell'episodio.

## Esecuzione del Sistema

Il codice principale ("glue code") mette in relazione le due classi, eseguendo il ciclo finché l'episodio non è concluso:


```Python
if __name__ == "__main__":
    env = Environment()
    agent = Agent()
    while not env.is_done():
        agent.step(env)
    print("Total reward got: %.4f" % agent.total_reward)
```

Questo schema rimane invariato indipendentemente dalla complessità dei componenti. L'ambiente potrebbe essere un sofisticato simulatore fisico e l'agente una rete neurale profonda, ma il protocollo di scambio (osservazione $\rightarrow$ azione $\rightarrow$ ricompensa) rappresenta lo standard universale del Reinforcement Learning.

# Requisiti Hardware e Software

Per affrontare correttamente gli esempi pratici di questo percorso nel Deep Reinforcement Learning, è necessario configurare un ambiente di sviluppo adeguato. La scelta tecnologica si è orientata verso strumenti open source che garantiscono flessibilità e prestazioni elevate.

## Ambiente di Sviluppo e Linguaggio

Tutti gli esempi sono stati implementati e testati utilizzando **Python 3.11**. Si presuppone una buona familiarità con il linguaggio e con concetti standard come gli ambienti virtuali (virtual environments). Un aspetto distintivo del codice presentato è l'uso intensivo delle **Type Annotations** (annotazioni di tipo), che migliorano la leggibilità e la manutenibilità definendo le firme di funzioni e metodi.

## Librerie Esterne Principali

Nonostante l'obiettivo sia mantenere le dipendenze al minimo per favorire implementazioni personalizzate, alcune librerie fondamentali sono indispensabili:

* **NumPy**: Essenziale per il calcolo scientifico e la manipolazione di matrici, alla base di ogni operazione matematica nel RL.
* **OpenCV (Python bindings)**: Utilizzata per la computer vision e l'elaborazione di immagini, fondamentale quando l'osservazione dell'agente è di tipo visuale.
* **Gymnasium (Farama Foundation)**: Un fork mantenuto della libreria OpenAI Gym. Fornisce un'interfaccia unificata per comunicare con una vasta gamma di ambienti.
* **PyTorch**: Il framework di Deep Learning (DL) scelto per la sua flessibilità. Gestisce la creazione e l'addestramento delle reti neurali.
* **PyTorch Ignite**: Un set di strumenti di alto livello che operano su PyTorch per ridurre il codice ripetitivo (*boilerplate*).
* **PTAN (PyTorch Agent Net)**: Un'estensione dell'API di Gymnasium creata specificamente per supportare i moderni metodi di Deep RL.

## Requisiti Hardware: Il Ruolo della GPU

Il Deep Reinforcement Learning è computazionalmente oneroso. L'uso di una **GPU (Graphics Processing Unit)** può accelerare i tempi di addestramento da 10 a 100 volte rispetto a un sistema basato esclusivamente su CPU.



Sebbene sia possibile eseguire il codice su una CPU, i tempi di addestramento potrebbero passare da poche ore a diversi giorni. Per chi non possiede una GPU locale compatibile con **CUDA**, esistono diverse alternative:
1.  **Istanze Cloud**: Servizi come AWS o Google Cloud offrono macchine dotate di GPU.
2.  **Google Colab**: Una piattaforma gratuita che fornisce accesso a GPU tramite notebook Jupyter.

## Specifiche del Sistema Operativo

I sistemi consigliati sono **Linux** o **macOS**. Sebbene Windows sia teoricamente supportato sia da PyTorch che da Gymnasium, la maggior parte dei test è stata condotta su ambienti Unix-like.

## File dei Requisiti (requirements.txt)

Per replicare l'ambiente di test, è possibile utilizzare le versioni specifiche delle librerie elencate di seguito:

```text
gymnasium[atari]==0.29.1
gymnasium[classic-control]==0.29.1
gymnasium[accept-rom-license]==0.29.1
moviepy==1.0.3
numpy<2
opencv-python==4.10.0.84
torch==2.5.0
torchvision==0.20.0
pytorch-ignite==0.5.1
tensorboard==2.18.0
mypy==1.8.0
ptan==0.8.1
stable-baselines3==2.3.2
torchrl==0.6.0
ray[tune]==2.37.0
pytest
```

# L'API OpenAI Gym e Gymnasium

La libreria Python chiamata **Gym** è stata originariamente sviluppata da OpenAI nel 2017. Da allora, un numero enorme di ambienti è stato adattato a questa API, che è diventata lo standard *de facto* per il Reinforcement Learning.

Nel 2021, il team di sviluppo originale è passato a **Gymnasium**, un fork mantenuto dalla Farama Foundation. Gymnasium funge da "sostituto immediato" (drop-in replacement) per la libreria originale: l'interfaccia rimane la stessa, garantendo compatibilità con il codice esistente tramite un semplice cambio di importazione.

## La Classe Centrale: `Env`

L'obiettivo principale di Gymnasium è fornire una collezione eterogenea di ambienti RL accessibili tramite un'interfaccia unificata. Il cuore della libreria è la classe `Env`, che espone metodi e attributi fondamentali per definire le capacità dell'ambiente.

Ogni ambiente Gymnasium fornisce le seguenti funzionalità chiave:

### 1. Spazio delle Azioni (Action Space)
L'ambiente definisce l'insieme di azioni che l'agente può eseguire. Gymnasium supporta diverse tipologie:
* **Azioni Discrete**: Un insieme finito di scelte (es. muoversi a destra o a sinistra).
* **Azioni Continue**: Valori numerici all'interno di un intervallo (es. la pressione esercitata su un acceleratore).
* **Combinazioni**: Strutture più complesse che uniscono entrambi i tipi.

### 2. Spazio delle Osservazioni (Observation Space)
L'ambiente specifica la forma (*shape*) e i limiti dei dati che l'agente riceve. Queste informazioni permettono all'agente di conoscere preventivamente i confini dei segnali sensoriali che dovrà elaborare.



### 3. Il Metodo `step`
Questo metodo è fondamentale per l'interazione dinamica. Quando l'agente sceglie un'azione $a_t$, la passa al metodo `step`, che restituisce quattro valori principali:
* **Osservazione**: Il nuovo stato percepito dall'agente.
* **Ricompensa**: Il valore scalare $r_t$ ottenuto in seguito all'azione.
* **Terminato/Troncato**: Indicatori booleani che segnalano se l'episodio è finito.
* **Info**: Un dizionario contenente informazioni ausiliarie per il debugging.

### 4. Il Metodo `reset`
Necessario per iniziare un nuovo episodio o ricominciare dopo una terminazione. Il metodo `reset` riporta l'ambiente al suo stato iniziale e restituisce la prima osservazione utile all'agente per prendere la decisione iniziale.

---

La standardizzazione di queste componenti permette agli sviluppatori di testare lo stesso agente su ambienti radicalmente diversi — da semplici giochi Atari a complesse simulazioni robotiche — senza dover riscrivere la logica di base della comunicazione tra agente e ambiente.

# Lo Spazio delle Azioni

All'interno del framework Gymnasium, le azioni che un agente può compiere sono classificate in base alla loro natura matematica e logica. Comprendere la struttura dello spazio delle azioni è fondamentale, poiché determina l'architettura dell'output della rete neurale dell'agente.

## Tipologie di Spazi delle Azioni

Le azioni possono essere raggruppate in tre categorie principali:

### 1. Azioni Discrete
Le azioni discrete rappresentano un insieme finito e numerabile di opzioni tra cui l'agente deve scegliere. In questo spazio, le opzioni si escludono a vicenda: l'agente può eseguire una sola azione in un determinato istante temporale $t$.

* **Esempi classici**: Muoversi in una griglia (su, giù, sinistra, destra) o lo stato di un pulsante (premuto, rilasciato).
* **Caratteristica principale**: Lo spazio è definito come un insieme $A = \{0, 1, \dots, n-1\}$, dove $n$ è il numero totale di azioni possibili.

### 2. Azioni Continue
Le azioni continue sono caratterizzate da valori numerici reali all'interno di un intervallo definito. A differenza delle azioni discrete, una singola azione continua può assumere infiniti valori tra i suoi limiti minimi e massimi.

* **Esempi classici**: L'angolo di rotazione di un volante o la pressione esercitata su un pedale dell'acceleratore.
* **Definizione dei limiti**: Ogni azione continua richiede la specifica dei confini (boundaries). Ad esempio, l'angolo di un volante potrebbe essere limitato nell'intervallo $[-720^\circ, 720^\circ]$, mentre la pressione di un pedale solitamente varia in $[0, 1]$.

### 3. Spazi Composti e Multipli
In scenari complessi, l'ambiente può richiedere l'esecuzione simultanea di più azioni. Ad esempio, un agente che guida un'auto potrebbe dover ruotare il volante e contemporaneamente premere l'acceleratore e il freno.

Per gestire queste situazioni, Gymnasium fornisce classi contenitore speciali (come `Box`, `Tuple` o `Dict`) che permettono di:
* **Combinare tipi diversi**: Unire azioni discrete e continue in un unico vettore di input.
* **Nidificare gli spazi**: Creare strutture gerarchiche che rappresentano la complessità delle decisioni dell'agente in modo unificato.

Formalmente, se abbiamo due spazi di azione $A_1$ e $A_2$, lo spazio risultante può essere visto come il prodotto cartesiano dei due set:

$$A_{total} = A_1 \times A_2$$

Questa flessibilità permette a Gymnasium di modellare qualsiasi cosa, dai semplici giochi logici ai sistemi robotici avanzati con numerosi gradi di libertà.

# Lo Spazio delle Osservazioni

Oltre alla ricompensa, l'ambiente fornisce all'agente informazioni sul proprio stato tramite le **osservazioni**. La complessità di queste informazioni può variare drasticamente: da un singolo valore booleano (come lo stato di una lampadina, acceso o spento) a tensori multidimensionali complessi contenenti immagini a colori provenienti da più telecamere.

## La Classe Astratta `Space`

In Gymnasium, sia le azioni che le osservazioni sono rappresentate tramite sottoclassi della classe astratta `Space`. Questa struttura uniforme permette di scrivere codice generico capace di interfacciarsi con diversi tipi di ambienti. Le proprietà e i metodi fondamentali di `Space` includono:

* **`shape`**: Indica la dimensione dello spazio, con una sintassi identica a quella degli array NumPy.
* **`sample()`**: Restituisce un campione casuale estratto dal dominio dello spazio.
* **`contains(x)`**: Verifica se un dato elemento $x$ appartiene al dominio dello spazio.
* **`seed()`**: Inizializza il generatore di numeri casuali per garantire la riproducibilità degli esperimenti.

## Sottoclassi Principali di `Space`

Esistono tre implementazioni principali che coprono la stragrande maggioranza dei casi d'uso nel Reinforcement Learning:

### 1. La Classe `Discrete`
Rappresenta un insieme di elementi mutuamente esclusivi, numerati da $0$ a $n-1$. È ideale per scenari con scelte binarie o direzionali. Ad esempio, `Discrete(n=4)` può rappresentare le quattro direzioni di movimento in una griglia.

### 2. La Classe `Box`
Rappresenta un tensore $n$-dimensionale di numeri razionali definiti in un intervallo $[low, high]$. È lo spazio standard per dati continui o dati visivi complessi:
* **Esempio scalare**: Un pedale dell'acceleratore con valori tra $0.0$ e $1.0$ si definisce come `Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)`.
* **Esempio visivo**: Un'immagine RGB di un gioco Atari ($210 \times 160$ pixel) viene rappresentata come `Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8)`. In questo caso, il tensore tridimensionale contiene i piani cromatici per il rosso, il verde e il blu.

### 3. La Classe `Tuple`
Questa classe funge da contenitore, permettendo di combinare diverse istanze di `Space`. È estremamente utile per modellare sistemi complessi con controlli eterogenei. Si consideri, ad esempio, un'auto che richiede simultaneamente:
* Input continui per volante e pedali (un'istanza `Box`).
* Input discreti per gli indicatori di direzione (un `Discrete(n=3)`: spento, destra, sinistra).
* Input discreti per il clacson (un `Discrete(n=2)`: on, off).

La flessibilità di `Tuple` permette di definire uno spazio d'azione o di osservazione di qualsiasi complessità strutturale.

## Altre Tipologie di Spazio

Sebbene meno comuni, Gymnasium supporta anche altri tipi di dati tramite classi dedicate:
* **`Sequence`**: Per sequenze di lunghezza variabile.
* **`Text`**: Per stringhe di testo.
* **`Graph`**: Per rappresentare stati strutturati come grafi (nodi e connessioni).

Ogni istanza di un ambiente Gymnasium espone due attributi fondamentali, `action_space` e `observation_space`, entrambi appartenenti a una delle sottoclassi di `Space`. Questa standardizzazione facilita l'integrazione di tecniche di Computer Vision (come le reti convoluzionali per spazi `Box` basati su immagini) o di algoritmi per spazi discreti all'interno di un unico framework coerente.

# La Classe Env: Il Motore dell'Ambiente

In Gymnasium, la classe `Env` funge da interfaccia astratta che definisce come l'agente interagisce con il mondo esterno. Essa incapsula la logica dello stato, la dinamica delle transizioni e il sistema di ricompensa attraverso alcuni metodi e attributi fondamentali.

## Attributi di Specifica

Ogni istanza di `Env` espone due membri di tipo `Space` che permettono all'agente di comprendere i propri confini operativi:
* **`action_space`**: Definisce il set di azioni consentite (es. Discrete, Box, etc.).
* **`observation_space`**: Definisce la struttura e i limiti delle informazioni sensoriali fornite.

## Il Metodo `reset()`

Il metodo `reset()` è il punto di partenza di ogni interazione. Esso deve essere chiamato obbligatoriamente subito dopo la creazione dell'ambiente o al termine di ogni episodio. 

L'obiettivo di `reset()` è riportare l'ambiente al suo stato iniziale $s_0$. Il metodo non accetta argomenti obbligatori e restituisce una tupla composta da:
1.  **L'osservazione iniziale**: Il primo vettore di dati che l'agente userà per decidere la prima azione.
2.  **Un dizionario `info`**: Contenente informazioni extra specifiche dell'ambiente (spesso vuoto negli ambienti standard).

## Il Metodo `step()`

Il metodo `step()` rappresenta l'atomo fondamentale dell'interazione agente-ambiente. Esso accetta come unico argomento l'azione scelta dall'agente e restituisce una tupla di cinque elementi che descrivono l'esito della transizione:

```python
observation, reward, terminated, truncated, info = env.step(action)
````

### Dettaglio dei Valori di Ritorno

- **`observation`**: Un vettore o una matrice NumPy che rappresenta il nuovo stato percepito dall'agente dopo aver eseguito l'azione.
    
- **`reward`**: Un valore scalare di tipo `float` che quantifica l'efficacia dell'azione intrapresa.
    
- **`terminated` (o `done`)**: Un booleano che indica se l'agente ha raggiunto uno stato terminale (es. "Game Over" o obiettivo raggiunto). Se `True`, è necessario chiamare `reset()`.
    
- **`truncated`**: Un booleano che indica se l'episodio è stato interrotto per motivi esterni alla logica di gioco (es. il raggiungimento di un limite di tempo massimo). Anche in questo caso è necessario il reset.
    
- **`info`**: Un dizionario Python con metadati ausiliari. Solitamente viene ignorato durante l'apprendimento, ma è utile per il debugging o per monitorare metriche specifiche.
    

## Il Ciclo di Interazione Standard

La struttura tipica di un programma di Reinforcement Learning segue un ciclo iterativo basato sui metodi sopra descritti. L'agente continua a invocare `step()` finché uno dei due flag di conclusione (`terminated` o `truncated`) non diventa vero.

Formalmente, l'obiettivo dell'agente è massimizzare la ricompensa cumulativa in un episodio:

$$G_t = R_{t+1} + R_{t+2} + \dots + R_T$$

Dove $T$ è l'istante temporale in cui `terminated` o `truncated` diventano `True`.

Oltre a questi metodi core, la classe `Env` offre utilità come `render()`, che permette di visualizzare l'ambiente in una forma comprensibile per l'uomo (grafica 2D/3D), sebbene non sia strettamente necessaria per la logica dell'algoritmo di apprendimento.

# Creazione di un Ambiente

In Gymnasium, il processo di istanziazione di un ambiente è standardizzato e avviene tramite un registro centrale che associa nomi univoci a specifiche configurazioni di simulazione.

## Nomenclatura e Versionamento

Ogni ambiente è identificato da una stringa nel formato `NomeAmbiente-vN`, dove $N$ rappresenta il numero di versione. Questo sistema permette di mantenere la riproducibilità degli esperimenti: se un bug viene corretto o la dinamica dell'ambiente cambia significativamente, viene rilasciata una nuova versione (ad esempio, passando da `-v0` a `-v4`).

Per creare un'istanza, si utilizza la funzione `make()` fornita dal pacchetto:

```python
import gymnasium as gym
env = gym.make("Breakout-v4")
````

## Varianti dello Stesso Ambiente

Un singolo gioco o simulazione può avere molteplici varianti nel registro. Prendendo come esempio il classico gioco Atari **Breakout**, esistono diverse configurazioni basate su come l'agente percepisce il mondo e come viene gestito il tempo:

- **Standard (`Breakout-v4`)**: Posizione e direzione della pallina iniziali casuali.
    
- **Deterministic**: Posizione e velocità iniziali della pallina fisse per ogni episodio.
    
- **NoFrameskip**: L'azione dell'agente viene applicata a ogni singolo frame. Senza questa opzione, Gymnasium applica solitamente la stessa azione per 2-4 frame consecutivi per velocizzare l'apprendimento.
    
- **RAM (`-ram`)**: L'osservazione fornita non è l'immagine dello schermo (pixel), ma lo stato della memoria RAM dell'emulatore Atari (128 byte).
    

## Categorie di Ambienti in Gymnasium

Gymnasium include centinaia di ambienti (circa 198 unici nella versione 0.29.1), classificabili in diverse categorie logiche:

|**Categoria**|**Descrizione**|**Esempi**|
|---|---|---|
|**Classic Control**|Problemi classici della teoria del controllo. Sono i "MNIST del Reinforcement Learning" per la loro semplicità.|CartPole, MountainCar|
|**Atari 2600**|Giochi arcade classici degli anni '70 e '80. Richiedono solitamente l'elaborazione di immagini (CNN).|Breakout, Pong, Space Invaders|
|**Box2D / MuJoCo**|Simulatori fisici per compiti di controllo continuo e robotica.|Camminata di umanoidi, guida di auto|
|**Toy Text**|Ambienti basati su griglie di testo semplici per testare la logica algoritmica.|FrozenLake, Taxi|
|**Algorithmic**|Compiti computazionali come copiare sequenze o sommare numeri.|Copy, DuplicatedInput|

Oltre a questi, esistono innumerevoli repository di terze parti che espongono l'interfaccia di Gymnasium per ambiti specializzati, come la navigazione 3D, l'automazione web o il reinforcement learning multi-agente.

# Esplorazione dell'Ambiente CartPole

Il **CartPole** è uno degli ambienti più iconici e semplici forniti da Gymnasium, appartenente al gruppo dei problemi di controllo classico. L'obiettivo del compito è bilanciare un'asta (pole) fissata tramite un perno a un carrello (cart) che si muove lungo un binario senza attrito.

Il sistema è intrinsecamente instabile: l'asta tende a cadere a destra o a sinistra a causa della gravità. L'agente deve applicare una forza orizzontale al carrello per mantenerla in equilibrio il più a lungo possibile.

## Lo Spazio delle Osservazioni

L'osservazione fornita dall'ambiente consiste in un vettore di quattro numeri in virgola mobile che descrivono lo stato cinematico del sistema ad ogni istante $t$:

1. **Posizione del carrello**: Valore nell'intervallo $[-4.8, 4.8]$.
    
2. **Velocità del carrello**: Valore nell'intervallo $(-\infty, \infty)$.
    
3. **Angolo dell'asta**: Misurato in radianti nell'intervallo circa $[-0.418, 0.418]$.
    
4. **Velocità angolare dell'asta**: Valore nell'intervallo $(-\infty, \infty)$.
    

Tecnicamente, Gymnasium rappresenta l'infinito utilizzando i valori massimi e minimi del tipo `float32`, che sono nell'ordine di $10^{38}$. Sebbene sia possibile utilizzare leggi fisiche per risolvere questo problema, la potenza del Reinforcement Learning risiede nel fatto che l'agente può imparare a bilanciare il sistema esclusivamente tramite **trial and error** (tentativi ed errori), senza conoscere il significato fisico di questi numeri.

## Lo Spazio delle Azioni e le Ricompense

L'ambiente `CartPole-v1` utilizza uno spazio di azioni di tipo `Discrete(2)`. Questo significa che l'agente ha a disposizione solo due scelte possibili in ogni passo:

- **Azione 0**: Spingere il carrello verso sinistra.
    
- **Azione 1**: Spingere il carrello verso destra.
    

Il sistema di ricompensa è estremamente semplice: l'agente riceve una ricompensa di $+1$ per ogni istante temporale in cui l'asta rimane in equilibrio. L'episodio termina non appena l'asta cade oltre una certa angolazione o il carrello esce dai limiti del binario. Pertanto, la massimizzazione della ricompensa totale $G = \sum R_t$ coincide con la massimizzazione della durata dell'episodio.

## Interazione Pratica con l'Ambiente

Per iniziare l'interazione, è necessario resettare l'ambiente per ottenere lo stato iniziale:

```Python
import gymnasium as gym
env = gym.make("CartPole-v1")
obs, info = env.reset()
```

L'invio di un'azione tramite il metodo `step()` restituisce la dinamica successiva del sistema:


```Python
next_obs, reward, terminated, truncated, info = env.step(0)
```

In questa sessione, se l'agente esegue l'azione $0$ (spinta a sinistra), riceverà un nuovo vettore di osservazione, una ricompensa di $1.0$ e i flag booleani `terminated` e `truncated` (che saranno `False` finché il palo è in equilibrio).

### Campionamento Casuale

Una funzionalità utile della classe `Space` è il metodo `sample()`. Invocare `env.action_space.sample()` restituisce in modo casuale $0$ o $1$. Sebbene un agente che sceglie azioni a caso non sia efficace nel lungo periodo, il campionamento è uno strumento fondamentale per le fasi iniziali di esplorazione in RL, permettendo all'agente di interagire con l'ambiente prima ancora di aver appreso una policy ottimale.

# L'Agente Random nel Problema CartPole

L'ambiente **CartPole** rappresenta un classico benchmark nel Reinforcement Learning, dove l'obiettivo è bilanciare un'asta verticale fissata su un carrello che si muove lungo un binario privo di attrito. Nonostante la complessità della dinamica fisica sottostante, l'implementazione di un agente tramite la libreria **Gymnasium** risulta estremamente concisa grazie all'uso di astrazioni standardizzate.

### Inizializzazione dell'Ambiente

```python
import gymnasium as gym

if __name__ == "__main__":
	env = gym.make("CartPole-v1")
	total_reward = 0.0
	total_steps = 0
	obs, _ = env.reset()
```

Il processo inizia con la creazione dell'istanza dell'ambiente tramite la funzione `gym.make("CartPole-v1")`. Prima di poter interagire con esso, è necessario preparare il sistema e le variabili di monitoraggio:

* **Reset dell'ambiente**: Il metodo `env.reset()` ripristina lo stato iniziale e restituisce la prima osservazione $s_0$.
* **Accumulatori**: Vengono inizializzate le variabili per il calcolo del premio totale $R$ e il conteggio dei passi temporali $t$, entrambi solitamente impostati a zero.

### Ciclo di Interazione e Logica dell'Agente

```python
while True:
	action = env.action_space.sample()
	obs, reward, is_done, is_trunc, _ = env.step(action)
	total_reward += reward
	total_steps += 1

	if is_done:
		break

print("Episode done in %d steps, total reward %.2f" % (total_steps,total_reward))
```

L'agente interagisce con l'ambiente all'interno di un ciclo continuo. In questa specifica implementazione, l'agente è di tipo **stocastico (random)**, il che significa che non utilizza le osservazioni per prendere decisioni, ma seleziona azioni in modo casuale.

1.  **Campionamento dell'azione**: Viene estratta un'azione $a_t$ dallo spazio delle azioni dell'ambiente utilizzando `env.action_space.sample()`.
2.  **Esecuzione del passo**: L'azione viene inviata all'ambiente tramite `env.step(action)`. Questo metodo è fondamentale poiché fa avanzare la simulazione di un passo temporale e restituisce una tupla di cinque valori:
    * **Osservazione** ($s_{t+1}$): Lo stato del sistema (posizione e velocità del carrello, angolo e velocità angolare dell'asta).
    * **Premio** ($r_t$): In CartPole, viene fornito un premio di $+1.0$ per ogni passo in cui l'asta rimane in equilibrio.
    * **Terminated** (`is_done`): Un flag booleano che diventa vero se l'asta cade oltre un certo angolo o il carrello esce dai limiti.
    * **Truncated** (`is_trunc`): Un flag che indica se l'episodio è terminato per aver raggiunto un limite massimo di passi (timeout).
    * **Info**: Un dizionario contenente informazioni ausiliarie per il debug.

### Analisi delle Prestazioni e Obiettivi

Le prestazioni di un agente vengono misurate in base al premio totale accumulato durante un episodio:

$$R = \sum_{t=1}^{T} r_t$$

Dove $T$ è il passo temporale finale. Un agente completamente casuale in CartPole solitamente ottiene un punteggio medio compreso tra $12$ e $15$ punti. 

Molti ambienti in Gymnasium definiscono un **reward boundary**, ovvero una soglia di punteggio medio che l'agente deve superare su $100$ episodi consecutivi per considerare l'ambiente come "risolto". Per la versione $v1$ di CartPole, questo limite è fissato a $195.0$. Di conseguenza, la strategia casuale risulta ampiamente insufficiente per risolvere il compito, fungendo però da base di confronto per algoritmi più avanzati.