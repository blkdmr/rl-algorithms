%----------------------------------------------------------------------------------------
%    PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[aspectratio=169,xcolor=dvipsnames]{beamer}
\usetheme{SimplePlus}

\usepackage{hyperref}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{amsmath}  % Added for math formatting
\usepackage{amssymb}  % Added for math symbols
\usepackage{algorithm} % Added for algorithms
\usepackage{algpseudocode} % Added for algorithms
\usepackage{hyperref} % Added for videos
\usepackage{multimedia}
\setbeamertemplate{footline}[frame number]

%----------------------------------------------------------------------------------------
%    TITLE PAGE
%----------------------------------------------------------------------------------------

\title{Reinforcement Learning-Based Agent Design}
\subtitle{Implementation in Gymnasium Environments}

% The presenter
\author{Alessio Russo}

\institute
{
    Department of Mathematical, Physical and Computer Sciences\\
    University of Parma
}
\date{}

%----------------------------------------------------------------------------------------
%    PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

\begin{document}

% Slide 1: Title
\begin{frame}[plain, noframenumbering]
    \titlepage
\end{frame}

%----------------------------------------------------------------------------------------
%    SECTION: BACKGROUND
%----------------------------------------------------------------------------------------

\begin{frame}{What is Reinforcement Learning?}

    Reinforcement learning is a learning paradigm based on trial-and-error interaction between an agent and an environment. In particular:

    \vspace{0.3cm}

    \begin{itemize}
        \item An agent observes the environment state, selects an action $a$, and receives a scalar reward $r$.
        \item The environment transitions to a new state $s$ in response to the action.
        \item The objective is to learn a policy that maximizes expected cumulative reward over time.
        \item Training data is generated online by the agent itself, not from a fixed dataset.
        \item Policies are commonly represented by neural networks trained using backpropagation.
    \end{itemize}

    \vspace{0.3cm}

    Observations may be partial or noisy representations of the true environment state.

\end{frame}

%------------------------------------------------

\begin{frame}{Key Challenges in Reinforcement Learning}

    Challenges specific to the reinforcement learning setting:
    \vspace{0.5cm}
    \begin{itemize}
        \item Data is non-independent and non-identically distributed, since the agent's policy determines which experiences are collected.
        \item Exploration is required to discover rewarding actions, but excessive exploration degrades short-term performance.
        \item Rewards are often delayed, making it difficult to associate actions with their long-term consequences.
    \end{itemize}

\end{frame}


%------------------------------------------------

%\begin{frame}{Markov Process (MP)}
%
%    A Markov Process (MP) is a mathematical model for systems that evolve randomly over time.
%
%    \vspace{0.25cm}
%
%    Markov property:
%    \[
%    P(s_{t+1} \mid s_t) = P(s_{t+1} \mid s_1, s_2, \dots, s_t)
%    \]
%
%    \vspace{0.25cm}
%
%    Interpretation:
%    \begin{itemize}
%        \item The current state is a complete summary of the past.
%        \item Knowing earlier states provides no additional predictive power.
%        \item The system evolves through probabilistic state transitions.
%        \item These transitions are specified by a transition matrix $T$, where
%        \[
%        T_{i,j} = P(S_{t+1} = j \mid S_t = i)
%        \]
%    \end{itemize}
%
%\end{frame}

%------------------------------------------------

%\begin{frame}{Markov Reward Process (MRP)}
%
%    A Markov Reward Process (MRP) builds on a MP by introducing a notion of desirability.
%
%    \vspace{0.25cm}
%
%    Interpretation:
%    \begin{itemize}
%        \item Each state or transition produces a scalar reward signal.
%        \item Rewards define what outcomes are considered good or bad.
%        \item Future rewards are discounted by a factor $\gamma$.
%        \item The value function summarizes long term expected reward.
%    \end{itemize}
%
%    \vspace{0.2cm}
%
%    \[
%    V(s) = \mathbb{E}[G_t \mid S_t = s]
%    \]
%
%\end{frame}

%------------------------------------------------

%\begin{frame}{Markov Decision Process (MDP)}
%
%A Markov Decision Process (MDP) extends a MRP by introducing actions that influence system dynamics.
%
%\vspace{0.2cm}
%
%At each time step:
%\begin{itemize}
%    \item The agent observes the current state.
%    \item The agent selects an action.
%    \item The environment transitions to a new state and emits a reward.
%\end{itemize}
%
%\vspace{0.15cm}
%
%Transitions and rewards depend on the chosen action:
%\[
%P(s' \mid s, a), \qquad R(s, a)
%\]
%
%\end{frame}

%------------------------------------------------

%\begin{frame}{Policy}
%
%The policy specifies how the agent behaves in each state.
%
%\vspace{0.2cm}
%
%Stochastic policy definition:
%\[
%\pi(a \mid s) = P(A_t = a \mid S_t = s)
%\]
%
%\vspace{0.2cm}
%
%Interpretation:
%\begin{itemize}
%    \item The policy maps states to probabilities over actions.
%    \item Stochasticity allows exploration of alternative behaviors.
%    \item Fixing the policy removes choice and yields a Markov Reward Process.
%\end{itemize}
%
%\end{frame}

%------------------------------------------------

\begin{frame}{Gymnasium: The RL Environment Standard}

Gymnasium specifies a common abstraction for reinforcement learning environments.

\vspace{0.25cm}

Environment abstraction:
\begin{itemize}
    \item An environment is exposed through a unified interface.
    \item Interaction follows a well defined agent environment loop.
    \item This separation allows algorithms to be reused across tasks.
\end{itemize}

\vspace{0.25cm}

Core elements:
\begin{itemize}
    \item Action space defines what decisions are available to the agent.
    \item Observation space defines how the environment state is perceived.
    \item \texttt{reset()} initializes an episode.
    \item \texttt{step(a)} advances the system by one interaction step.
\end{itemize}

\end{frame}

%----------------------------------------------------------------------------------------
%    SECTION: MCTS on CartPole
%----------------------------------------------------------------------------------------

\begin{frame}{Monte Carlo Tree Search (MCTS)}

Core Principle:

\begin{itemize}
    \item Build a partial search tree starting from the current state.
    \item Evaluate actions by simulating future trajectories and observing their cumulative rewards.
    \item Focus on actions that empirically lead to better long-term outcomes.
\end{itemize}

\vspace{0.25cm}

Search Mechanism:

\begin{itemize}
    \item Each node represents a state, and each outgoing edge corresponds to a possible action.
    \item Action selection balances exploring uncertain actions and exploiting actions with high estimated value.
    \item Simulation results are backpropagated through the tree to progressively refine action value estimates.
\end{itemize}

\end{frame}


%------------------------------------------------

\begin{frame}{MCTS applied to CartPole}

Task setting:
\begin{itemize}
    \item Control a cart to keep an attached pole balanced in an upright position.
    \item Performance is measured by episode duration.
\end{itemize}

\vspace{0.2cm}

Environment description:
\begin{itemize}
    \item Observations are low dimensional state vectors describing cart and pole dynamics.
    \item State variables include cart position and velocity, and pole angle and angular velocity.
    \item Actions correspond to discrete left or right forces applied to the cart.
\end{itemize}

\vspace{0.2cm}

Reward design:
\begin{itemize}
    \item The agent receives a constant positive reward at each time step.
    \item Reward accumulation directly reflects how long the pole remains balanced.
    \item Episodes terminate when the pole falls or the cart leaves the allowed range.
\end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}{MCTS demonstration on CartPole}

    \centering
    \movie[
        width=0.5\textwidth,
        height=0.375\textwidth,
        showcontrols
    ]{\includegraphics[width=0.5\textwidth]{images/preview/cart_pole.png}}{videos/cart_pole.mp4}

    \vspace{0.2cm}

    \small MCTS controlled agent interacting with the CartPole environment

\end{frame}

%----------------------------------------------------------------------------------------
%    SECTION: CEM on LunarLander
%----------------------------------------------------------------------------------------

\begin{frame}{Cross Entropy Method (CEM)}

    Core idea:
    \begin{itemize}
        \item Maintain a probability distribution over candidate solutions.
        \item Sample multiple solutions and evaluate them by running full episodes.
        \item Select the highest performing samples as elite.
        \item Update the distribution to increase the likelihood of elite solutions.
    \end{itemize}

    \vspace{0.25cm}

    Use in reinforcement learning:
    \begin{itemize}
        \item The policy is represented by a set of parameters.
        \item Policies are sampled, executed in the environment, and scored by total reward.
        \item Only top performing policies influence the next parameter distribution.
        \item Repeating this process progressively concentrates on better policies.
    \end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}{CEM applied to LunarLander}

    Task setting:
    \begin{itemize}
        \item Control a spacecraft to achieve a safe and fuel efficient landing.
        \item Performance is evaluated over complete episodes.
    \end{itemize}

    \vspace{0.2cm}

    Environment description:
    \begin{itemize}
        \item Observations encode position, velocity, orientation, and ground contact.
        \item Actions correspond to discrete engine commands controlling thrust.
    \end{itemize}

    \vspace{0.2cm}

    Reward design:
    \begin{itemize}
        \item Continuous shaping encourages stable descent and correct positioning.
        \item Fuel usage is penalized to discourage unnecessary thrust.
        \item Successful landings are strongly rewarded, crashes are heavily penalized.
    \end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}{CEM configuration for LunarLander}

    Model architecture:
    \begin{itemize}
        \item Input: low-dimensional state vector (flattened environment observation).
        \item Feature mapping: compact MLP with ReLU nonlinearities to capture state interactions.
        \item Policy head: final linear layer producing action logits for discrete control.
        \item Output: categorical action distribution over the 4 LunarLander actions.
    \end{itemize}

    \vspace{0.2cm}

    Training and sampling:
    \begin{itemize}
        \item Adam optimizer with learning rate $10^{-3}$ and cross-entropy loss.
        \item 1000 episodes sampled per epoch; top 20\% retained by return.
        \item Episodes truncated at 300 steps; 10 optimization steps per epoch.
    \end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}{Training results for LunarLander}

\centering
\includegraphics[width=0.45\textwidth]{images/results/lunar_lander.png}

\vspace{0.2cm}
\small Training performance of CEM on the LunarLander environment

\end{frame}

%------------------------------------------------

\begin{frame}{CEM demonstration on LunarLander}

    \centering
    \movie[
        width=0.5\textwidth,
        height=0.375\textwidth,
        showcontrols
    ]{\includegraphics[width=0.5\textwidth]{images/preview/lunar_lander.png}}{videos/lunar_lander.mp4}

    \vspace{0.2cm}

    \small CEM controlled agent interacting with the LunarLander environment

\end{frame}

%----------------------------------------------------------------------------------------
%    SECTION: CEM on Galaxian
%----------------------------------------------------------------------------------------

\begin{frame}{CEM applied to Galaxian}

    Task setting:
    \begin{itemize}
        \item Control a spacecraft to survive enemy waves and score points.
        \item Performance is evaluated over complete game episodes.
    \end{itemize}

    \vspace{0.2cm}

    Environment description:
    \begin{itemize}
        \item Observations are raw RGB images representing the full game screen.
        \item Visual input encodes player position, enemies, bullets, and background.
        \item Actions correspond to discrete controls such as moving left, moving right, and firing.
    \end{itemize}

    \vspace{0.2cm}

    Reward design:
    \begin{itemize}
        \item Positive reward is given for destroying enemies.
        \item Survival is encouraged by avoiding terminal states.
        \item Episodes terminate when the player ship is destroyed.
    \end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}{CEM configuration for Galaxian}

    Model architecture:
    \begin{itemize}
        \item Input: single RGB frame resized to $96 \times 96$ and normalized per channel.
        \item Feature extractor: deep convolutional network capturing spatial structure of Atari frames.
        \item Policy head: fully connected layers mapping visual features to action logits.
        \item Output: categorical action distribution over the discrete Galaxian action space.
    \end{itemize}

    \vspace{0.2cm}

    Training and sampling:
    \begin{itemize}
        \item Adam optimizer with learning rate $10^{-3}$ and cross-entropy loss.
        \item 300 episodes sampled per epoch; top 20\% retained by return.
        \item Episodes truncated at 400 steps; 20 optimization steps per epoch.
    \end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}{Training results for Galaxian}

\centering
\includegraphics[width=0.45\textwidth]{images/results/galaxian.png}

\vspace{0.2cm}
\small Training performance of CEM on the Galaxian environment

\end{frame}

%------------------------------------------------

\begin{frame}{CEM demonstration on Galaxian}

    \centering
    \movie[
        width=0.5\textwidth,
        height=0.375\textwidth,
        showcontrols
    ]{\includegraphics[width=0.5\textwidth]{images/preview/galaxian.png}}{videos/galaxian.mp4}

    \vspace{0.2cm}

    \small CEM controlled agent interacting with the Galaxian environment

\end{frame}

%----------------------------------------------------------------------------------------
%    SECTION: THANK-YOU SECTION
%----------------------------------------------------------------------------------------

\begin{frame}[plain, noframenumbering]
    \centering

    \Large{\centerline{\textbf{Thank you for your attention!}}}
    \vspace{1.2cm}

    \footnotesize
    \textit{Alessio Russo}\\
    \textit{University of Parma}

    \vspace{0.6cm}

    \footnotesize
    Code: \texttt{https://github.com/blkdmr/rl-algorithms}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document}
